# 5.1 层与块

层的定义：

（1）接受一组输入

（2）生成相应的输出

（3）由一组可调整参数描述。

下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层， 然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。

```python
import torch
from torch import nn
from torch.nn import functional as F

# Sequential顺序的加入三层
# nn.Linear(intputs, outputs)  第一个参数是输入，第二个是输出
net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

# rand表示随机生成一个2x20的矩阵
# python中的*是解包的作用
X = torch.rand(2, 20)
net(X)  # net(X)实际调用了__call__(X),其中__call__(X) 又调用了forward函数
```

在这个例子中，我们通过实例化`nn.Sequential`来构建我们的模型， 层的执行顺序是作为参数传递的。 简而言之，`nn.Sequential`定义了一种特殊的`Module`， 即在PyTorch中表示一个块的类， 它维护了一个由`Module`组成的有序列表。 注意，两个全连接层都是`Linear`类的实例， `Linear`类本身就是`Module`的子类。 另外，到目前为止，我们一直在通过`net(X)`调用我们的模型来获得模型的输出。 这实际上是`net.__call__(X)`的简写。 这个前向传播函数非常简单： 它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。

### 5.1.1 自定义块

每个块必须提供的基本功能：

1. 将输入数据作为其前向传播函数的参数。  X
2. 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。
3. 计算其输出(Y)关于输入(X)的梯度，可通过其反向传播函数**Y.backward()**进行访问。通常这是自动发生的。
4. 存储和访问前向传播计算所需的参数。
5. 根据需要初始化模型参数。

在下面的代码片段中，我们从零开始编写一个块。 它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。 注意，下面的`MLP`类继承了表示块的类。 我们的实现只需要提供我们自己的构造函数（Python中的`__init__`函数）和前向传播函数。

```python
class MLP(nn.Module):  # 表示MLP继承至nn.Module
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):  # 初始化参数
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))
    
net = MLP()  # 生成一个实例对象
net(X)   # 进行调用，实际上调用的是forward函数
```

### 5.1.2 顺序块

`Sequential`的设计是为了把其他模块串起来

 我们可以自己构建一个简化的`MySequential`，只需要定义两个关键函数：

1. 一种将块逐个追加到列表中的函数。
2. 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。

```python
class MySequential(nn.Module):  # 继承至nn.Module类
    def __init__(self, *args):  # *args表示可变参数
        super().__init__()  # 进行初始化
        # enumerate(args)将返回args参数的索引及下标 
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。module的类型是OrderedDict
            # str(idx)表示将idx转换为字符串
            # 将其添加至modules字典中
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            # 即是将上一模型的输出作为下一模型的输入，依次执行下去
            X = block(X)
        return X
# 进行调用，即是依次将nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10)加入到_modules中，最后在forward中进行调用
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
# MySequential中的参数，最后都会被放到*args中去
net(X)  # net中的X参数将会调用MySequential中的forward函数
```

`__init__`函数将每个模块逐个添加到有序字典`_modules`中，`_modules`的主要优点是： 在模块的参数初始化过程中， 系统知道在`_modules`字典中查找需要初始化参数的子块。

### 5.1.3 在前向传播函数中执行代码

有时我们可能希望合并既不是上一层的结果也不是可更新参数的项， 我们称之为*常数参数*（constant parameter），例如，我们需要一个计算函数 **$f(x,w)=c⋅wx$​**的层， 其中$x$​是输入， $w$​是参数， $c$​是某个在优化过程中没有更新的指定常量。

```python
class FixedHiddenMLP(nn.Module):  # 继承至nn.Module父类
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        # requires_grad=False 表示不计算梯度,即权重在训练期间保持不变
        # rand 是一个(0,1)的均匀分布，返回(0,1)的数值
        # randn是一个正态分布，mean=0, std=1
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)  # 调用nn.Linear(20, 20)函数
        # 使用创建的常量参数以及relu和mm函数
        # torch.mm表示的是矩阵乘法  1x2与2x3->1x3
        # mm对应的时matrix*matrix
        # torch.mul(a, b)是矩阵a和b对应位相乘 即此时矩阵的大小必须一致
        # 下面的+1是对计算之后的X所有元素都加上1
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()  # 返回的是一个标量
    
net = FixedHiddenMLP()
net(X)
```

可以混合搭配各种组合块的方法。 在下面的例子中，我们以一些想到的方法嵌套块。

```python
class NestMLP(nn.Module):
    def __init__(self):  # 相当于无参构造器
        super().__init__() # 调用父类的构造器进行初始化
        # net算一个大块 linear算另外一块
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

# Sequential中的一个参数为一个块
chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

### 5.1.4 总结

- 一个块可以由许多层组成；一个块可以由许多块组成。
- 块可以包含代码。
- 块负责大量的内部处理，包括参数初始化和反向传播。
- 层和块的顺序连接由`Sequential`块处理。



# 5.2. 参数管理

我们训练的目标是找到使损失函数最小化的模型参数值。经过训练后，我们将需要使用这些参数来做出未来的预测。 此外，有时我们希望提取参数，以便在其他环境中复用它们， 将模型保存下来，以便它可以在其他软件中执行， 或者为了获得科学的理解而进行检查。

- 访问参数，用于调试、诊断和可视化。
- 参数初始化。
- 在不同模型组件间共享参数。

我们首先看一下具有单隐藏层的多层感知机。

```python
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))  # rand表示(0,1)的均匀分布
net(X)
```

## 5.2.1. 参数访问

从已有模型中访问参数。 当通过`Sequential`类定义模型时， 我们可以通过索引来访问模型的任意层。 这就像模型是一个列表一样，每层的参数都在其属性中。

```python
print(net[2].state_dict())  # 打印参数的字典列表
```

输出的结果告诉我们一些重要的事情： 首先，这个全连接层包含两个参数，分别是该层的权重和偏置。 两者都存储为单精度浮点数（float32）。 注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。

### 5.2.1.1. 目标参数

注意，每个参数都表示为参数类的一个实例。 要对参数执行任何操作，首先我们需要访问底层的数值。 有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。 下面的代码从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。

```python
print(type(net[2].bias))  # net[2].bias的参数类型为torch.nn.parameter.Parameter
print(net[2].bias)  # 打印bias的全部东西，其中包括了数据以及梯度
print(net[2].bias.data) # 打印net[2].bias.data只需要数据
```

参数是复合的对象，包含值、梯度和额外信息。 除了值之外，我们还可以访问每个参数的梯度。 在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。

```python
net[2].weight.grad == None
```

### 5.2.1.2. 一次性访问所有参数

当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。 当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂， 因为我们需要递归整个树来提取每个子块的参数。

```python
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
print(*[(name, param.shape) for name, param in net.named_parameters()])
# *作用在列表中，主要是起到解包的作用
# 
net.state_dict()['2.bias'].data  # state_dict得到的是一个字典['2.bias']根据值取数据
```

### 5.2.1.3. 从嵌套块收集参数

让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。 我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。

```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                         nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        # 在这里嵌套
        net.add_module(f'block {i}', block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
rgnet(X) # block2中包括了4个block1，每一个block1中包括了4层卷积

print(rgnet) # 可以看到网络的内层构造

# result数据显示
Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
```

因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。 下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。

```python
rgnet[0][1][0].bias.data 

# 数据展示
(block 1): Sequential(
       # 实际访问的->数据
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
```

## 5.2.2. 参数初始化

默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。 PyTorch的`nn.init`模块提供了多种预置初始化方法。

### 5.2.2.1. 内置初始化

首先调用内置的初始化器。 下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0。

```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
        
# model.apply(fn)会递归地将函数fn应用到父模块的每个子模块submodule
# 加入type(m) == nn.Linear可以对某些特定的子模块submodule做一些针对性的处理
net.apply(init_normal)  # 表示对net中的nn.Linear模块进行权重初始化
net[0].weight.data[0], net[0].bias.data[0]

# 将nn.Linear的参数初始化为给定的常数
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)
net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]


# 还可以对某些块应用不同的初始化方法。 例如，下面我们使用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42。

def xavier(m):
    if type(m) == nn.Linear:
        # https://blog.csdn.net/luoxuexiong/article/details/95772045
        # 上面的博客讲解了xavier_uniform_的初始化过程
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(xavier) # 即对net[0]中的nn.Linear模块的权重进行初始化
net[2].apply(init_42)
print(net[0].weight.data[0])
print(net[2].weight.data)
```

### 5.2.2.2. 自定义初始化

![image-20211210155044506](http://cdn.nefu-yzk.top/img/image-20211210155044506.png)

```python
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape)
                        for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)  # 表示(-10, 10)的均匀分布
        # m.weight.data.abs() >= 5 对于m中的数据返回的是false或者true
        # 相乘之后，只有两者都为true才为true
        m.weight.data *= m.weight.data.abs() >= 5

net.apply(my_init)  # 因为net中有两个nn.Linear层,所以其中的if语句执行两次
net[0].weight[:2]

# 注意，我们始终可以直接设置参数。
net[0].weight.data[:] += 1
net[0].weight.data[0, 0] = 42
net[0].weight.data[0]
```

## 5.2.3. 参数绑定

有时我们希望在多个层间共享参数： 我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。

```python
# 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)  # 共享层
# Sequential一个参数就是一层
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))

net(X)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 确保它们实际上是同一个对象，而不仅仅只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])
```

这个例子表明第三个和**第五个神经网络层**的参数是绑定的。 它们不仅值相等，而且由相同的张量表示。 因此，如果我们改变其中一个参数，另一个参数也会改变。 你可能会思考：当参数绑定时，梯度会发生什么情况？ 答案是由于模型参数包含梯度，**因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起**。

共享参数通常可以节省内存，并在以下方面具有特定的好处：

- 对于图像识别中的CNN，共享参数使网络能够在图像中的任何地方而不是仅在某个区域中查找给定的功能。
- 对于RNN，它在序列的各个时间步之间共享参数，因此可以很好地推广到不同序列长度的示例。
- 对于自动编码器，编码器和解码器共享参数。 在具有线性激活的单层自动编码器中，共享权重会在权重矩阵的不同隐藏层之间强制正交。

# 5.3. 延后初始化

到目前为止，我们忽略了建立网络时需要做的以下这些事情：

- 我们定义了网络架构，但没有指定输入维度。
- 我们添加层时没有指定前一层的输出维度。
- 我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。

你可能会对我们的代码能运行感到惊讶。 毕竟，深度学习框架无法判断网络的输入维度是什么。 这里的诀窍是框架的*延后初始化*（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。

当使用卷积神经网络时， 由于输入维度（即图像的分辨率）将影响每个后续层的维数， 有了该技术将更加方便。 现在我们在编写代码时无须知道维度是什么就可以设置参数， 这种能力可以大大简化定义和修改模型的任务。

但PyTorch中一开始就需要指定输入维度

# 5.4. 自定义层

## 5.4.1. 不带参数的层

构造一个没有任何参数的自定义层。下面的`CenteredLayer`类要从其输入中减去均值。 要构建它，我们只需继承基础层类并实现前向传播功能。

```python
import torch
import torch.nn.functional as F
from torch import nn


class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()  # 未进行本类的初始化，仅仅调用父类的初始化操作

    def forward(self, X):
        return X - X.mean()  # X中的每个数都减去均值
    
layer = CenteredLayer()
layer(torch.FloatTensor([1, 2, 3, 4, 5]))

# 现在，我们可以将层作为组件合并到更复杂的模型中。
net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())

# 作为额外的健全性检查，我们可以在向该网络发送随机数据后，检查均值是否为0。 由于我们处理的是浮点数，因为存储精度的原因，我们仍然可能会看到一个非常小的非零数。
Y = net(torch.rand(4, 8))
Y.mean()
```

## 5.4.2. 带参数的层

以上我们知道了如何定义简单的层，下面我们继续定义具有参数的层， 这些参数可以通过训练进行调整。 我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。 比如管理访问、初始化、共享、保存和加载模型参数。 这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。

现在，让我们实现自定义版本的全连接层。 回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。 在此实现中，我们使用修正线性单元作为激活函数。 该层需要输入参数：`in_units`和`units`，分别表示输入数和输出数。

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        # randn表示mean=0, std=1的标准正态分布
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        # torch.matmul是点乘，即是对应元素相乘，此时矩阵的大小需要一致
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
# 5 3将对应到__init__中的in_units和units参数上
linear = MyLinear(5, 3)
linear.weight

# 直接执行forward函数
linear(torch.rand(2, 5))

# 可以使用自定义层构建模型，就像使用内置的全连接层一样使用自定义层。
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
net(torch.rand(2, 64))
```

# 5.5. 读写文件

当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。

## 5.5.1. 加载和保存张量

对于单个张量，我们可以直接调用`load`和`save`函数分别读写它们。 这两个函数都要求我们提供一个名称，`save`要求将要保存的变量作为输入。

```python
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)  # 声明一个Tensor
torch.save(x, 'x-file')  # 将x进行存储，存储的文件名为x-file

x2 = torch.load("x-file")  # 将数据读回内存中

# 我们可以存储一个张量列表，然后把它们读回内存。
y = torch.zeros(4)
torch.save([x, y], "x-files")
x2, y2 = torch.load("x-files")

# 我们甚至可以写入或读取从字符串映射到张量的字典。 当我们要读取或写入模型中的所有权重时，这很方便。
mydict = {'x': x, 'y': y}
torch.save(mydict, "mydict")
mydict2 = torch.load("mydict")
mydict2
```

## 5.5.2. 加载和保存模型参数

保存单个权重向量（或其他张量）确实有用， 但是如果我们想保存整个模型，并在以后加载它们， 单独保存每个向量则会变得很麻烦。 毕竟，我们可能有数百个参数散布在各处。 因此，深度学习框架提供了内置函数来保存和加载整个网络。 需要注意的一个重要细节是，这将**保存模型的参数而不是保存整个模型**。 例如，如果我们有一个3层多层感知机，我们需要单独指定架构。 因为模型本身可以包含任意代码，所以模型本身难以序列化。 因此，为了恢复模型，我们需要用代码生成架构， 然后从磁盘加载参数。 让我们从熟悉的多层感知机(MLP)开始尝试一下。

```python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)

# 接下来，我们将模型的参数存储在一个叫做“mlp.params”的文件中。
torch.save(net.state_dict(), 'mlp.params')
# net的state_dict()中存放着参数


#为了恢复模型，我们实例化了原始多层感知机模型的一个备份。 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数。
clone = MLP()  # 实例化一个MLP对象
clone.load_state_dict(torch.load('mlp.params'))  # 进行模型参数的加载
clone.eval()  # 进入评估模式，即是不调用dropout函数

# 由于两个实例具有相同的模型参数，在输入相同的X时， 两个实例的计算结果应该相同。 让我们来验证一下。
Y_clone = clone(X)
Y_clone == Y
```

## 5.5.3. 小结

- `save`和`load`函数可用于张量对象的文件读写。
- 我们可以通过参数字典保存和加载网络的全部参数。`save(net.state_dict(), "parmas")`
- 保存架构必须在代码中完成，而不是在参数中完成。



# 5.6. GPU

```python
!nvidia-smi # 查看显卡信息
```

在PyTorch中，每个数组都有一个设备（device）， 我们通常将其称为上下文（context）。 默认情况下，所有变量和相关的计算都分配给CPU。 有时上下文可能是GPU。

## 5.6.1. 计算设备

我们可以指定用于存储和计算的设备，如CPU和GPU。 默认情况下，张量是在内存中创建的，然后使用CPU计算它。

在PyTorch中，CPU和GPU可以用`torch.device('cpu')` 和`torch.device('cuda')`表示。 应该注意的是，`cpu`设备意味着所有物理CPU和内存， 这意味着PyTorch的计算将尝试使用所有CPU核心。 然而，`gpu`设备只代表一个卡和相应的显存。 如果有多个GPU，我们使用`torch.device(f'cuda:{i}')` 来表示第ii块GPU（ii从0开始）。 另外，`cuda:0`和`cuda`是等价的。

```python
import torch
from torch import nn

# cuda:0和cuda是等价的
torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')

# 查询GPU的数量
torch.cuda.device_count()

# #@save表示将该函数存入d2l包中
def try_gpu(i=0):  #@save  
    """如果存在，则返回gpu(i)，否则返回cpu()"""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpus():  #@save
    """返回所有可用的GPU，如果没有GPU，则返回[cpu(),]"""
    # 使用了列表推导式
    devices = [torch.device(f'cuda:{i}')
             for i in range(torch.cuda.device_count())]
    # if devices为true则返回devices，否则返回torch.device("cpu")
    return devices if devices else [torch.device('cpu')]

try_gpu(), try_gpu(10), try_all_gpus()
```



## 5.6.2. 张量与GPU



```python
# 可以查询张量所在的设备  x.device
x = torch.tensor([1, 2, 3])
x.device
# 对不同的张量进行计算时，需要保证都位于同一个设备上
```

需要注意的是，无论何时我们要对多个项进行操作， 它们都必须在同一个设备上。 例如，如果我们对两个张量求和， 我们需要确保两个张量都位于同一个设备上， 否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。

### 5.6.2.1. 存储在GPU上

有几种方法可以在GPU上存储张量。 例如，我们可以在创建张量时指定存储设备。接 下来，我们在第一个`gpu`上创建张量变量`X`。 在GPU上创建的张量只消耗这个GPU的显存。 我们可以使用`nvidia-smi`命令查看显存使用情况。 一般来说，我们需要确保不创建超过GPU显存限制的数据。

```python
# 创建张量时指定设备
X = torch.ones(2, 3, device = try_gpu())
X
```

### 5.6.2.2. 复制

![image-20211210184109179](http://cdn.nefu-yzk.top/img/image-20211210184109179.png)

```python
Z = X.cuda(1)  # 即是将X移到cuda:1中去
print(X)
print(Z)


# 如果移动动张量本身已在目标位置，则直接进行返回
Z.cuda(1) is Z
# 上面语句将返回True
```

## 5.6.3. 神经网络与GPU

类似地，神经网络模型可以指定设备。 下面的代码将模型参数放在GPU上。

```python
net = nn.Sequential(nn.Linear(3, 1))  # 指定神经网络模型
net = net.to(device=try_gpu())  # 将net移到GPU中去

net(X)  # 该计算将在GPU上进行

net[0].weight.data.device  # 查看模型参数所处的位置是不是在同一个GPU上
```

## 5.6.4. 小结

- 我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。
- 深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。
- 不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy `ndarray`中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。