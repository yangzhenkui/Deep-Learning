在本章中的每一个模型都曾一度占据主导地位，其中许多模型都是ImageNet竞赛的优胜者。ImageNet竞赛自2010年以来，一直是计算机视觉中监督学习进展的指向标。

这些模型包括：

- AlexNet。它是第一个在大规模视觉竞赛中击败传统计算机视觉模型的大型神经网络；
- 使用**重复块**的网络（VGG）。它利用许多重复的**神经网络块**；
- 网络中的网络（NiN）。它重复使用由卷积层和**1×1卷积层**（用来代替全连接层）来构建深层网络;
- 含并行连结的网络（GoogLeNet）。它使用并行连结的网络，通过不同窗口大小的卷积层和最大汇聚层来并行抽取信息；
- 残差网络（ResNet）。它通过残差块构建跨层的数据通道，是计算机视觉中最流行的体系架构；
- 稠密连接网络（DenseNet）。它的计算成本很高，但给我们带来了更好的效果。

# 7.1. 深度卷积神经网络（AlexNet）

卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。

因此，与训练*端到端*（从像素到分类结果）系统不同，经典机器学习的流水线看起来更像下面这样：

1. 获取一个有趣的数据集。在早期，收集这些数据集需要昂贵的传感器（在当时最先进的图像也就100万像素）。
2. 根据光学、几何学、其他知识以及偶然的发现，手工对特征数据集进行预处理。
3. 通过标准的特征提取算法，如SIFT（尺度不变特征变换）和SURF（加速鲁棒特征）或其他手动调整的流水线来输入数据。
4. 将提取的特征送入最喜欢的分类器中（例如线性模型或其它核方法），以训练分类器。

图像识别的诡异事实————**推动领域进步的是数据特征，而不是学习算法。**

从对最终模型精度的影响来说，更大或更干净的数据集、或是稍微改进的特征提取，比任何学习算法带来的进步要大得多。

数据特征决定了模型的上限，而算法不过是不断地逼近该上限而已。

## 7.1.1. 学习表征

![image-20211212103740688](http://cdn.nefu-yzk.top/img/image-20211212103740688.png)

### 7.1.1.1. 缺少的成分：数据

![image-20211212103858294](http://cdn.nefu-yzk.top/img/image-20211212103858294.png)

### 7.1.1.2. 缺少的成分：硬件

![image-20211212104223230](http://cdn.nefu-yzk.top/img/image-20211212104223230.png)

## 7.1.2. AlexNet

AlexNet使用了8层卷积神经网络，并以很大的优势赢得了2012年ImageNet图像识别挑战赛。

AlexNet和LeNet的架构非常相似，如 [图7.1.2](https://zh-v2.d2l.ai/chapter_convolutional-modern/alexnet.html#fig-alexnet)所示。 注意，这里我们提供了一个稍微精简版本的AlexNet，去除了当年需要两个小型GPU同时运算的设计特点。

![image-20211212104545999](http://cdn.nefu-yzk.top/img/image-20211212104545999.png)

### 7.1.2.1. 模型设计

![image-20211212105100026](http://cdn.nefu-yzk.top/img/image-20211212105100026.png)

### 7.1.2.2. 激活函数

激活函数的主要作用是为了原来的线性改成非线性

![image-20211212105456922](http://cdn.nefu-yzk.top/img/image-20211212105456922.png)

### 7.1.2.3. 容量控制和预处理

AlexNet通过dropout（ [4.6节](https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/dropout.html#sec-dropout)）控制全连接层的模型复杂度，而LeNet只使用了权重衰减。 为了进一步扩充数据，**AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色**。 这使得模型更健壮，**更大的样本量有效地减少了过拟合**。 我们将在 [13.1节](https://zh-v2.d2l.ai/chapter_computer-vision/image-augmentation.html#sec-image-augmentation)中更详细地讨论数据扩充。

```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    # 这里，我们使用一个11*11的更大窗口来捕捉对象。
    # 同时，步幅为4，以减少输出的高度和宽度。
    # 另外，输出通道的数目远大于LeNet
    # (n - k + p + s) / s
    # 每一次的输出大小都可以经过计算出来的
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # 使用三个连续的卷积层和较小的卷积窗口。
    # 除了最后的卷积层，输出通道的数量进一步增加。
    # 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),
    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合
    nn.Linear(6400, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
    nn.Linear(4096, 10))


# 构造一个高度和宽度都为224的单通道数据，来观察每一层输出的形状。 它与 图7.1.2中的AlexNet架构相匹配。
# 样本数、通道数
X = torch.randn(1, 1, 224, 224)
for layer in net:
    X=layer(X)
    print(layer.__class__.__name__,'output shape:\t',X.shape)
```

## 7.1.3. 读取数据集

![image-20211212113708084](http://cdn.nefu-yzk.top/img/image-20211212113708084.png)

## 7.1.4. 训练AlexNet

现在，我们可以开始训练AlexNet了。与 [6.6节](https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/lenet.html#sec-lenet)中的LeNet相比，这里的主要变化是使用更小的学习速率训练，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。

```python
lr, num_epochs = 0.01, 10
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

![image-20211212113951075](http://cdn.nefu-yzk.top/img/image-20211212113951075.png)

## 7.1.5. 小结

- AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。
- 今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。
- 尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。
- Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。

# 7.2. 使用块的网络（VGG）

## 7.2.1. VGG块

![image-20211212114349440](http://cdn.nefu-yzk.top/img/image-20211212114349440.png)



```python
import torch
from torch import nn
from d2l import torch as d2l

# 该函数有三个参数，分别对应于卷积层的数量num_convs、输入通道的数量in_channels 和输出通道的数量out_channels.
def vgg_block(num_convs, in_channels, out_channels):
    layers = []  # 定义每一层的架构
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels  # 上一层的输出作为下一层的输入
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    return nn.Sequential(*layers)  # *layers中*的作用是解包
```

## 7.2.2. VGG网络

与AlexNet、LeNet一样，VGG网络可以分为两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。如 [图7.2.1](https://zh-v2.d2l.ai/chapter_convolutional-modern/vgg.html#fig-vgg)中所示。

![image-20211212114836503](http://cdn.nefu-yzk.top/img/image-20211212114836503.png)

VGG神经网络连续连接 [图7.2.1](https://zh-v2.d2l.ai/chapter_convolutional-modern/vgg.html#fig-vgg)的几个VGG块（在`vgg_block`函数中定义）。其中有超参数变量`conv_arch`。该变量指定了每个VGG块里卷积层个数和输出通道数。全连接模块则与AlexNet中的相同。

原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。 第一个模块有64个输出通道，每个后续模块将输出通道数量翻倍，直到该数字达到512。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。  11表示8个卷积层+3个全连接层

每一个卷积层都是先卷积，再进行激活函数

```python
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))
```

下面的代码实现了VGG-11。可以通过在`conv_arch`上执行for循环来简单实现。

```python
def vgg(conv_arch):
    conv_blks = []
    in_channels = 1
    # 卷积层部分
    for (num_convs, out_channels) in conv_arch:
        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
        in_channels = out_channels

    return nn.Sequential(
        *conv_blks, nn.Flatten(),
        # 全连接层部分
        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 10))

net = vgg(conv_arch)

# 构建一个高度和宽度为224的单通道数据样本，以观察每个层输出的形状。
X = torch.randn(size=(1, 1, 224, 224))
for blk in net:
    X = blk(X)
    print(blk.__class__.__name__,'output shape:\t',X.shape)
    
# 每个块的高度和宽度减半，最终高度和宽度都为7。最后再展平表示，送入全连接层处理。
```

## 7.2.3. 训练模型

由于VGG-11比AlexNet计算量更大，因此我们构建了一个通道数较少的网络，足够用于训练Fashion-MNIST数据集。

```python
ratio = 4
small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]
net = vgg(small_conv_arch)  # // 表示地板除

# 除了使用略高的学习率外，模型训练过程与 7.1节中的AlexNet类似。
lr, num_epochs, batch_size = 0.05, 10, 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

![image-20211212120100975](http://cdn.nefu-yzk.top/img/image-20211212120100975.png)

## 7.2.4. 小结

- VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。
- 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。
- 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即3×3）比较浅层且宽的卷积更有效。



# 7.5. 批量规范化

*批量规范化*（batch normalization）可持续加速深层网络的收敛速度。

## 7.5.1. 训练深层网络

批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先**规范化输入**，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。 接下来，我们应用比例系数和比例偏移。 正是由于这个基于*批量*统计的*标准化*，才有了*批量规范化*的名称。

![image-20211212122135952](http://cdn.nefu-yzk.top/img/image-20211212122135952.png)

![image-20211212122628916](http://cdn.nefu-yzk.top/img/image-20211212122628916.png)

![image-20211212122837079](http://cdn.nefu-yzk.top/img/image-20211212122837079.png)

## 7.5.2. 批量规范化层

批量规范化和其他图层之间的一个关键区别是，由于批量规范化在完整的小批次上运行，因此我们不能像以前在引入其他图层时那样忽略批处理的尺寸大小。 我们在下面讨论这两种情况：全连接层和卷积层，他们的批量规范化实现略有不同。

### 7.5.2.1. 全连接层

![image-20211212123204108](http://cdn.nefu-yzk.top/img/image-20211212123204108.png)

> **注：上图中的仿射变换不是卷积运算**

### 7.5.2.2. 卷积层

![image-20211212123502844](http://cdn.nefu-yzk.top/img/image-20211212123502844.png)

### 7.5.2.3. 预测过程中的批量规范化

正如我们前面提到的，批量规范化在训练模式和预测模式下的行为通常不同。 首先，将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。 其次，例如，我们可能需要使用我们的模型对逐个样本进行预测。 一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。 可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。

## 7.5.3. 从零实现

我们从头开始实现一个具有张量的批量规范化层。

```python
import torch
from torch import nn
from d2l import torch as d2l


# moving_mean和moving_var是该批次中全部通道的平均值与方差  eps是干扰项
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式
    if not torch.is_grad_enabled():
        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)  # 判断X是二维数据还是四维数据 
        # 2表示完全连接层，4表示卷积层
        if len(X.shape) == 2:
            # 使用全连接层的情况，计算特征维上的均值和方差
            mean = X.mean(dim=0)  # dim表示按行求和 |  eg: nxn->1xn
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。
            # 这里我们需要保持X的形状以便后面可以做广播运算
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # 训练模式下，用当前的均值和方差做标准化
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 更新移动平均的均值和方差
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # 缩放和移位
    return Y, moving_mean.data, moving_var.data
```

我们现在可以创建一个正确的`BatchNorm`图层。 这个层将保持适当的参数：拉伸`gamma`和偏移`beta`,这两个参数将在训练过程中更新。 此外，我们的图层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。

撇开算法细节，注意我们实现图层的基础设计模式。 通常情况下，我们用一个单独的函数定义其数学原理，比如说`batch_norm`。 然后，我们将此功能集成到一个自定义层中，其代码主要处理**簿记问题**，例如将数据移动到训练设备（如GPU）、分配和初始化任何必需的变量、跟踪移动平均线（此处为均值和方差）等。 为了方便起见，我们并不担心在这里自动推断输入形状，因此我们需要指定整个特征的数量。 不用担心，深度学习框架中的批量规范化API将为我们解决上述问题，我们稍后将展示这一点。

> “簿记问题”是指一些事务性工作，不一定要跟某个特定算法直接相关。

```python
class BatchNorm(nn.Module):
    # num_features：完全连接层的输出数量或卷积层的输出通道数。
    # num_dims：2表示完全连接层，4表示卷积层
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # 非模型参数的变量初始化为0和1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # 如果X不在内存上，将moving_mean和moving_var
        # 复制到X所在显存上
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # 保存更新过的moving_mean和moving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y
```

## 7.5.4. 使用批量规范化层的 LeNet

为了更好理解如何应用`BatchNorm`，下面我们将其应用于LeNet模型（ [6.6节](https://zh-v2.d2l.ai/chapter_convolutional-neural-networks/lenet.html#sec-lenet)）。 回想一下，批量规范化是在卷积层或全连接层之后、相应的激活函数之前应用的。

```python
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5), BatchNorm(6, num_dims=4), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), BatchNorm(16, num_dims=4), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),
    nn.Linear(16*4*4, 120), BatchNorm(120, num_dims=2), nn.Sigmoid(),
    nn.Linear(120, 84), BatchNorm(84, num_dims=2), nn.Sigmoid(),
    nn.Linear(84, 10))

# 和以前一样，我们将在Fashion-MNIST数据集上训练网络。 这个代码与我们第一次训练LeNet（ 6.6节）时几乎完全相同，主要区别在于学习率大得多。
lr, num_epochs, batch_size = 1.0, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())

# 让我们来看看从第一个批量规范化层中学到的拉伸参数gamma和偏移参数beta。
net[1].gamma.reshape((-1,)), net[1].beta.reshape((-1,))
```

## 7.5.5. 简明实现

使用框架中自带的BatchNorm

```python
net = nn.Sequential(
    # 只需要指定完全连接层的输出数量或卷积层的输出通道数
    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),
    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),
    nn.Linear(84, 10))

# 我们使用相同超参数来训练模型。 请注意，通常高级API变体运行速度快得多，因为它的代码已编译为C++或CUDA，而我们的自定义代码由Python实现。
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```



## 7.5.7. 小结

- 在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。
- 批量规范化在全连接层和卷积层的使用略有不同。
- 批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。
- 批量规范化有许多有益的副作用，主要是正则化。另一方面，”减少内部协变量偏移“的原始动机似乎不是一个有效的解释。



# 7.6. 残差网络（ResNet）

## 7.6.1. 函数类

![image-20211212131733354](http://cdn.nefu-yzk.top/img/image-20211212131733354.png)

![image-20211212132154402](http://cdn.nefu-yzk.top/img/image-20211212132154402.png)

## 7.6.2. 残差块

![image-20211212132549012](http://cdn.nefu-yzk.top/img/image-20211212132549012.png)

ResNet沿用了VGG完整的3×3卷积层设计。 残差块里首先有2个有相同输出通道数的3×3卷积层。 每个卷积层后接一个批量规范化层和ReLU激活函数。 然后我们**通过跨层数据通路**，**跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前**。 这样的设计要求2个卷积层的输出与输入形状一样，从而使它们可以相加。 如果想改变通道数，就需要引入**一个额外的1×1卷积层**(权重维度为输出x输入通道数)来将输入变换成需要的形状后再做相加运算。 残差块的实现如下：

```python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l


# 残差是为了解决网络退化
# 常做的操作是高宽减半，通道数加倍
class Residual(nn.Module):  #@save
    # num_channels为输出通道数
    def __init__(self, input_channels, num_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        if use_1x1conv:  # 1x1卷积层主要是改变通道数
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        # 为什么要用两个？
        # 因为每一个BatchNorm2d都有自己的参数需要去学习
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))  #没有使用激活函数
        if self.conv3:  
            # 使用1x1卷积层的目的主要是将输入的通道数改成符合条件的通道数，作用在最初的输入上
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
```

![image-20211212133113619](http://cdn.nefu-yzk.top/img/image-20211212133113619.png)

下面我们来查看输入和输出形状一致的情况。

```python
blk = Residual(3,3)
# 4各样本，每一个通道为3
X = torch.rand(4, 3, 6, 6)
Y = blk(X)
Y.shape  # torch.Size([4, 3, 6, 6])

# 我们也可以在增加输出通道数的同时，减半输出的高和宽。
blk = Residual(3,6, use_1x1conv=True, strides=2)
blk(X).shape  # torch.Size([4, 6, 3, 3])
```

## 7.6.3. ResNet模型

ResNet的前两层跟之前介绍的GoogLeNet中的一样： 在输出通道数为64、步幅为2的7×7卷积层后，接步幅为2的3×3的最大汇聚层。 不同之处在于ResNet每个卷积层后增加了批量规范化层。

```python
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.BatchNorm2d(64), nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
```

GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。 第一个模块的通道数同输入通道数一致。 由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。 之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。

```python
# 下面我们来实现这个模块。注意，我们对第一个模块做了特别处理。
def resnet_block(input_channels, num_channels, num_residuals,
                 first_block=False):
    blk = []
    for i in range(num_residuals):  # residuals是残差块
        if i == 0 and not first_block:
            # 是第一个残差块的话高宽就不减半了，因为前面已经减掉了
            # 第一个残差块使用1x1卷积进行改变输出通道数
            blk.append(Residual(input_channels, num_channels,
                                use_1x1conv=True, strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
    return blk

# 接着在ResNet加入所有残差块，这里每个模块使用2个残差块。
# b2中的第一个残差块不进行减半，其他的都进行减半操作
# **表示将list解开   **表示将dict解开
b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))  # 第一个块
b3 = nn.Sequential(*resnet_block(64, 128, 2))
b4 = nn.Sequential(*resnet_block(128, 256, 2))
b5 = nn.Sequential(*resnet_block(256, 512, 2))

# 最后，与GoogLeNet一样，在ResNet中加入全局平均汇聚层，以及全连接层输出。
net = nn.Sequential(b1, b2, b3, b4, b5,
                    nn.AdaptiveAvgPool2d((1,1)),
                    nn.Flatten(), nn.Linear(512, 10))
```

每个模块有4个卷积层（不包括恒等映射的1×1卷积层）。 加上第一个7×7卷积层和最后一个全连接层，共有18层。 因此，这种模型通常被称为ResNet-18。 通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。 虽然ResNet的主体架构跟GoogLeNet类似，但ResNet架构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。 [图7.6.4](https://zh-v2.d2l.ai/chapter_convolutional-modern/resnet.html#fig-resnet18)描述了完整的ResNet-18。

![image-20211212134306748](http://cdn.nefu-yzk.top/img/image-20211212134306748.png)

在训练ResNet之前，让我们观察一下ResNet中不同模块的输入形状是如何变化的。 在之前所有架构中，分辨率降低，通道数量增加，直到全局平均汇聚层聚集所有特征。

```python
X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
```

## 7.6.4. 训练模型

同之前一样，我们在Fashion-MNIST数据集上训练ResNet。

```python
lr, num_epochs, batch_size = 0.05, 10, 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

![image-20211212134454668](http://cdn.nefu-yzk.top/img/image-20211212134454668.png)

## 7.6.5. 小结

- 学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。
- 残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。
- 利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。
- 残差网络（ResNet）对随后的深层神经网络设计产生了深远影响。

![image-20211212192149764](http://cdn.nefu-yzk.top/img/image-20211212192149764.png)

保证梯度到深度网络的时候继续保持住梯度的值

ResNet的优点：

1 加深模型可以退化为浅层模型
2 梯度高速通道